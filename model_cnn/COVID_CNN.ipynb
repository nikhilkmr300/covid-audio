{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "COVID_CNN_New.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0MgWrrctmxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "114da5a0-6909-4004-97c4-073c8a1deafd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo1a_2XNtwUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d12aa08-3ca2-43b0-be6e-7034061039fd"
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/SOP/SOP_COVID/covid-audio/voice"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/SOP/SOP_COVID/covid-audio/voice\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_92-4oDtcEq"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalMaxPooling2D, Dropout\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from keras.layers.normalization import BatchNormalization"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUKQtq6ntcEv"
      },
      "source": [
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "audioType = 'breath'\n",
        "\n",
        "classes = ['covid', 'normal']\n",
        "\n",
        "target_size = (224, 224)\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 100\n",
        "learning_rate = 1e-5\n",
        "dropout_prob_1 = 0.25\n",
        "dropout_prob_2 = 0.5\n",
        "\n",
        "# Early stopping callback\n",
        "es = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0.001,\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "train_path = f'data_spec/mel_spec_new/spec_{audioType}/train'\n",
        "valid_path = f'data_spec/mel_spec_new/spec_{audioType}/valid'\n",
        "test_path = f'data_spec/mel_spec_new/spec_{audioType}/test'\n",
        "\n",
        "# Imbalanced dataset\n",
        "# class_weight = {\n",
        "#     0 : 4.,\n",
        "#     1 : 1.\n",
        "# }"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU2R_RowtcEz"
      },
      "source": [
        "# CNN Based Model in Keras\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(224,224,3)))\n",
        "model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128,(3,3),activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# model.add(GlobalMaxPooling2D())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1,activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSqbdXyo6522"
      },
      "source": [
        "# # CNN Based Model from research paper\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=(224,224,3)))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# model.add(Conv2D(32,(3,3),activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# model.add(Conv2D(128,(3,3),activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# model.add(Conv2D(256,(3,3),activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# model.add(Conv2D(512,(3,3),activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# model.add(GlobalMaxPooling2D())  # this converts our 3D feature maps to 1D feature vectors\n",
        "# model.add(Dense(1,activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t30dXHqftcE2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "269bde92-81eb-456e-d8a6-cd2492c95d8d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 220, 220, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 110, 110, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 110, 110, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 108, 108, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 52, 52, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 26, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 86528)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                5537856   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,668,097\n",
            "Trainable params: 5,668,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXDdOyzZtcE7"
      },
      "source": [
        "# Train from scratch\n",
        "train_datagen = image.ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    # horizontal_flip = True,\n",
        ")\n",
        "\n",
        "test_datagen = image.ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-AdG1RHtcE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8cf22a0-e568-4def-fded-e7dc7732197d"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size = target_size,\n",
        "    color_mode=\"rgb\", # for coloured images\n",
        "    batch_size = 32,\n",
        "    class_mode = 'binary'\n",
        ")\n",
        "\n",
        "print(train_generator.class_indices)\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    valid_path,\n",
        "    target_size = target_size,\n",
        "    color_mode=\"rgb\", # for coloured images\n",
        "    batch_size = 32,\n",
        "    class_mode = 'binary'\n",
        ")\n",
        "\n",
        "# Choosing steps per epoch so that 1 epoch equals 1 pass through train/valid/test (steps = samples / batch_size).\n",
        "train_batch_size = 32\n",
        "train_steps = round(train_generator.n / train_batch_size)\n",
        "valid_batch_size = 32\n",
        "valid_steps = round(validation_generator.n / valid_batch_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 287 images belonging to 2 classes.\n",
            "{'covid': 0, 'normal': 1}\n",
            "Found 37 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd1cntubrAgj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e34ed3b9-2aa2-4af7-b85a-9dbe1fab4953"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
        "    loss=keras.losses.binary_crossentropy,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_steps,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=valid_steps,\n",
        "    callbacks=[es]\n",
        ")\n",
        "\n",
        "model_name = f'model_CNN_{audioType}_epochs_lr_new'\n",
        "model.save('models/' + model_name + '.h5')\n",
        "\n",
        "with open(f'history/{model_name}_history.pickle', 'wb') as f:\n",
        "    pickle.dump(history.history, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2/9 [=====>........................] - ETA: 0s - loss: 0.6952 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0457s vs `on_train_batch_end` time: 0.0738s). Check your callbacks.\n",
            "9/9 [==============================] - 5s 573ms/step - loss: 0.7021 - accuracy: 0.4739 - val_loss: 0.6922 - val_accuracy: 0.6250\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 5s 521ms/step - loss: 0.6991 - accuracy: 0.5087 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 5s 519ms/step - loss: 0.6864 - accuracy: 0.5505 - val_loss: 0.6928 - val_accuracy: 0.4375\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 5s 517ms/step - loss: 0.6920 - accuracy: 0.5296 - val_loss: 0.6903 - val_accuracy: 0.5625\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 5s 526ms/step - loss: 0.6801 - accuracy: 0.6063 - val_loss: 0.6878 - val_accuracy: 0.6875\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 5s 531ms/step - loss: 0.6943 - accuracy: 0.5226 - val_loss: 0.6881 - val_accuracy: 0.7500\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 5s 527ms/step - loss: 0.6746 - accuracy: 0.5749 - val_loss: 0.6877 - val_accuracy: 0.7500\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 5s 527ms/step - loss: 0.6759 - accuracy: 0.6098 - val_loss: 0.6834 - val_accuracy: 0.5625\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 5s 523ms/step - loss: 0.6816 - accuracy: 0.5540 - val_loss: 0.6859 - val_accuracy: 0.6562\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 5s 541ms/step - loss: 0.6906 - accuracy: 0.5505 - val_loss: 0.6852 - val_accuracy: 0.6562\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 5s 525ms/step - loss: 0.6758 - accuracy: 0.5819 - val_loss: 0.6827 - val_accuracy: 0.7188\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 5s 524ms/step - loss: 0.6726 - accuracy: 0.5714 - val_loss: 0.6808 - val_accuracy: 0.6875\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 5s 521ms/step - loss: 0.6718 - accuracy: 0.5889 - val_loss: 0.6845 - val_accuracy: 0.7812\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 5s 525ms/step - loss: 0.6736 - accuracy: 0.5610 - val_loss: 0.6839 - val_accuracy: 0.6250\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 5s 529ms/step - loss: 0.6566 - accuracy: 0.6098 - val_loss: 0.6788 - val_accuracy: 0.5938\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 5s 521ms/step - loss: 0.6519 - accuracy: 0.6028 - val_loss: 0.6723 - val_accuracy: 0.6875\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 5s 521ms/step - loss: 0.6441 - accuracy: 0.6167 - val_loss: 0.6780 - val_accuracy: 0.6562\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 5s 524ms/step - loss: 0.6516 - accuracy: 0.6341 - val_loss: 0.6707 - val_accuracy: 0.6562\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 5s 527ms/step - loss: 0.6516 - accuracy: 0.6202 - val_loss: 0.6662 - val_accuracy: 0.6250\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 5s 535ms/step - loss: 0.6384 - accuracy: 0.6063 - val_loss: 0.6665 - val_accuracy: 0.6562\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 5s 535ms/step - loss: 0.6495 - accuracy: 0.6167 - val_loss: 0.6646 - val_accuracy: 0.6875\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 5s 539ms/step - loss: 0.6450 - accuracy: 0.5819 - val_loss: 0.6598 - val_accuracy: 0.6875\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 5s 526ms/step - loss: 0.6423 - accuracy: 0.5958 - val_loss: 0.6633 - val_accuracy: 0.6562\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 5s 520ms/step - loss: 0.6228 - accuracy: 0.6620 - val_loss: 0.6595 - val_accuracy: 0.6562\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 5s 524ms/step - loss: 0.6361 - accuracy: 0.6376 - val_loss: 0.6550 - val_accuracy: 0.6875\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 5s 520ms/step - loss: 0.6214 - accuracy: 0.6167 - val_loss: 0.6600 - val_accuracy: 0.7188\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 5s 522ms/step - loss: 0.6245 - accuracy: 0.6551 - val_loss: 0.6379 - val_accuracy: 0.7500\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 5s 535ms/step - loss: 0.6222 - accuracy: 0.6969 - val_loss: 0.6322 - val_accuracy: 0.7188\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 5s 550ms/step - loss: 0.6116 - accuracy: 0.6551 - val_loss: 0.6547 - val_accuracy: 0.6562\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 5s 528ms/step - loss: 0.6095 - accuracy: 0.7038 - val_loss: 0.6450 - val_accuracy: 0.6562\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 5s 519ms/step - loss: 0.6315 - accuracy: 0.6237 - val_loss: 0.6530 - val_accuracy: 0.6875\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 5s 527ms/step - loss: 0.6119 - accuracy: 0.6655 - val_loss: 0.6523 - val_accuracy: 0.7500\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 5s 532ms/step - loss: 0.6176 - accuracy: 0.6376 - val_loss: 0.6562 - val_accuracy: 0.6250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUnjbIZDvfl_"
      },
      "source": [
        "## Test Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSgmTP1ZvfmD"
      },
      "source": [
        "model_name = f'model_CNN_{audioType}_epochs_lr_new'\n",
        "model = load_model('models/' + model_name + '.h5')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0JRaD4fvfmg"
      },
      "source": [
        "y_actual = []\n",
        "y_test = []"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExLQ9bhiFFZb"
      },
      "source": [
        "for class_ in classes:\n",
        "    class_path = os.path.join(test_path, class_)\n",
        "    for i in os.listdir(class_path):\n",
        "        img = image.load_img(os.path.join(class_path, i), target_size=(224,224))\n",
        "        img = image.img_to_array(img)\n",
        "        img = np.expand_dims(img, axis = 0)\n",
        "        p = model.predict_classes(img)\n",
        "        y_test.append(p[0])\n",
        "        y_actual.append(train_generator.class_indices[class_])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ARk3BRmFJw4"
      },
      "source": [
        "y_actual = np.array(y_actual)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQhXad8aFMMo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "07b248d7-b0c7-4e5d-9143-2ffba0b3b248"
      },
      "source": [
        "cm = confusion_matrix(y_actual, y_test)\n",
        "print(cm)\n",
        "sns.heatmap(cm, cmap = 'plasma', annot = True, fmt = \".1f\")\n",
        "print('Accuracy: ', accuracy_score(y_actual, y_test))\n",
        "print(classification_report(y_actual, y_test))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[15  3]\n",
            " [62 18]]\n",
            "Accuracy:  0.336734693877551\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.19      0.83      0.32        18\n",
            "           1       0.86      0.23      0.36        80\n",
            "\n",
            "    accuracy                           0.34        98\n",
            "   macro avg       0.53      0.53      0.34        98\n",
            "weighted avg       0.74      0.34      0.35        98\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdYzVpwHs0Ef"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}